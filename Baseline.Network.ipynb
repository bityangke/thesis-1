{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "traindir = '/home/saniea/imagedata/kerastut/train/'\n",
    "valdir = '/home/saniea/imagedata/kerastut/validate/'\n",
    "testdir = '/home/saniea/imagedata/kerastut/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is this is the basic network. Works without any pretrained network. Works with very little data, only 2000\\nimages per class to train and 400 per class to validate\\nreaches 86% validation accuracy\\nTODO: figure out how to test this network using images from test data\\nTODO: figure out how to expand problem to more than two classes\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This is this is the basic network. Works without any pretrained network. Works with very little data, only 2000\n",
    "images per class to train and 400 per class to validate\n",
    "reaches 86% validation accuracy\n",
    "TODO: figure out how to test this network using images from test data\n",
    "'''\n",
    "\n",
    "# batch_size = 16\n",
    "\n",
    "# # this is the augmentation configuration we will use for training\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#         rescale=1./255,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True)\n",
    "\n",
    "\n",
    "# # this is the augmentation configuration we will use for testing:\n",
    "# # only rescaling\n",
    "# val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "# # this is a generator that will read pictures found in\n",
    "# # subfolers of 'data/train', and indefinitely generate\n",
    "# # batches of augmented image data\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#         traindir,  # this is the target directory\n",
    "#         target_size=(150, 150),  # all images will be resized to 150x150\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# # this is a similar generator, for validation data\n",
    "# validation_generator = val_datagen.flow_from_directory(\n",
    "#         valdir,\n",
    "#         target_size=(150, 150),\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='binary')\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# # the model so far outputs 3D feature maps (height, width, features)\n",
    "\n",
    "# model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "# model.add(Dense(64))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit_generator(\n",
    "#         train_generator,\n",
    "#         steps_per_epoch=2000 // batch_size,\n",
    "#         epochs=50,\n",
    "#         validation_data=validation_generator,\n",
    "#         validation_steps=800 // batch_size)\n",
    "# model.save_weights('first_try.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis part of the code uses the weights generated from the bottleneck feature usage in the previous part and the just\\nfine tunes the top CNN block of the ImageNet network. Weights for the whole CNN block are generated, 4/5 blocks are\\nfrozen to prevent any further changes. After this, our own top layer is place on the network, weights are trained and\\ngenerated\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This part of the code uses the weights generated from the bottleneck feature usage in the previous part and the just\n",
    "fine tunes the top CNN block of the ImageNet network. Weights for the whole CNN block are generated, 4/5 blocks are\n",
    "frozen to prevent any further changes. After this, our own top layer is place on the network, weights are trained and\n",
    "generated\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottleneck_features():\n",
    "    \n",
    "    files = os.listdir(\".\")\n",
    "    for file in files:\n",
    "        if file.startswith('bottleneck'):\n",
    "            os.remove(file)\n",
    "            \n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    # build VGG16 network\n",
    "    vgg_model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
    "    print(\"VGG Model Loaded\")\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "            traindir,\n",
    "            target_size=(150, 150),\n",
    "            batch_size=batch_size,\n",
    "            class_mode=None,  # this means our generator will only yield batches of data, no labels\n",
    "            shuffle=False)  # our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n",
    "\n",
    "    # the predict_generator method returns the output of a model, given\n",
    "    # a generator that yields batches of numpy data\n",
    "\n",
    "    bottleneck_features_train = vgg_model.predict_generator(generator, 3000//batch_size)\n",
    "\n",
    "    # save the output as a Numpy array\n",
    "    np.save(open('bottleneck_features_train', 'wb'), bottleneck_features_train)\n",
    "    print(\"Training features generated and saved\")\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "            valdir,\n",
    "            target_size=(150, 150),\n",
    "            batch_size=batch_size,\n",
    "            class_mode=None,\n",
    "            shuffle=False)\n",
    "    \n",
    "    bottleneck_features_validation = vgg_model.predict_generator(generator, 1200//batch_size)\n",
    "    np.save(open('bottleneck_features_validation', 'wb'), bottleneck_features_validation)\n",
    "    \n",
    "    print(\"Validation features generated and saved\")\n",
    "\n",
    "    return vgg_model\n",
    "\n",
    "def train_weights():\n",
    "    \n",
    "    # the features were saved in order, so recreating the labels is easy\n",
    "    train_data = np.load(open('bottleneck_features_train','rb'))\n",
    "    train_labels = np.array([0] * 1000 + [1] * 1000 + [2] * 1000)\n",
    "    train_labels = to_categorical(train_labels)\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation', 'rb'))\n",
    "    validation_labels = np.array([0] * 400 + [1] * 400 + [2] * 400)\n",
    "    validation_labels = to_categorical(validation_labels)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=60,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "    print(\"Top model weights trained\")\n",
    "\n",
    "def build_top_model(vgg_model):\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=vgg_model.output_shape[1:]))\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "    # note that it is necessary to start with a fully-trained\n",
    "    # classifier, including the top classifier,\n",
    "    # in order to successfully do fine-tuning\n",
    "    top_model.load_weights(top_model_weights_path)\n",
    "    print(\"Top model built\")\n",
    "\n",
    "    return top_model\n",
    "\n",
    "def composite_model(vgg_model, top_model):\n",
    "    # # add the model on top of the convolutional base\n",
    "    # model.add(top_model)\n",
    "    model=Model(input=vgg_model.input, output=top_model(vgg_model.output))\n",
    "    print(\"Final model built\")\n",
    "\n",
    "    # set the first 15 layers (up to the last conv block)\n",
    "    # to non-trainable (weights will not be updated)\n",
    "    for layer in model.layers[:15]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile the model with a SGD/momentum optimizer\n",
    "    # and a very slow learning rate.\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=1e-6, momentum=0.9),\n",
    "                  metrics=['categorical_accuracy'])\n",
    "   \n",
    "    # prepare data augmentation configuration\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        traindir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        valdir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    print(\"Beginning model fitting\")\n",
    "    # fine-tune the model\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=3000//batch_size,\n",
    "        epochs=60,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=1200//batch_size)\n",
    "        \n",
    "    model.summary()\n",
    "    \n",
    "    with open('model_history', 'wb') as f:\n",
    "        pickle.dump(hist.history, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG Model Loaded\n",
      "Found 3000 images belonging to 3 classes.\n",
      "Training features generated and saved\n",
      "Found 1200 images belonging to 3 classes.\n",
      "Validation features generated and saved\n",
      "Train on 3000 samples, validate on 1200 samples\n",
      "Epoch 1/60\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0377 - categorical_accuracy: 0.3630 - val_loss: 1.0975 - val_categorical_accuracy: 0.3108\n",
      "Epoch 2/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.9702 - categorical_accuracy: 0.5037 - val_loss: 0.6887 - val_categorical_accuracy: 0.7925\n",
      "Epoch 3/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.5392 - categorical_accuracy: 0.7997 - val_loss: 0.4206 - val_categorical_accuracy: 0.8908\n",
      "Epoch 4/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.2949 - categorical_accuracy: 0.8800 - val_loss: 0.3101 - val_categorical_accuracy: 0.9058\n",
      "Epoch 5/60\n",
      "3000/3000 [==============================] - 1s 340us/step - loss: 0.1828 - categorical_accuracy: 0.9327 - val_loss: 0.3797 - val_categorical_accuracy: 0.8792\n",
      "Epoch 6/60\n",
      "3000/3000 [==============================] - 1s 372us/step - loss: 0.1278 - categorical_accuracy: 0.9447 - val_loss: 0.2720 - val_categorical_accuracy: 0.9017\n",
      "Epoch 7/60\n",
      "3000/3000 [==============================] - 1s 340us/step - loss: 0.0945 - categorical_accuracy: 0.9653 - val_loss: 0.3209 - val_categorical_accuracy: 0.9058\n",
      "Epoch 8/60\n",
      "3000/3000 [==============================] - 1s 338us/step - loss: 0.0715 - categorical_accuracy: 0.9747 - val_loss: 0.2542 - val_categorical_accuracy: 0.9133\n",
      "Epoch 9/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0532 - categorical_accuracy: 0.9777 - val_loss: 0.3747 - val_categorical_accuracy: 0.8942\n",
      "Epoch 10/60\n",
      "3000/3000 [==============================] - 1s 331us/step - loss: 0.0372 - categorical_accuracy: 0.9863 - val_loss: 0.3706 - val_categorical_accuracy: 0.9117\n",
      "Epoch 11/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0476 - categorical_accuracy: 0.9817 - val_loss: 0.3981 - val_categorical_accuracy: 0.9075\n",
      "Epoch 12/60\n",
      "3000/3000 [==============================] - 1s 334us/step - loss: 0.0333 - categorical_accuracy: 0.9883 - val_loss: 0.6242 - val_categorical_accuracy: 0.8733\n",
      "Epoch 13/60\n",
      "3000/3000 [==============================] - 1s 335us/step - loss: 0.0329 - categorical_accuracy: 0.9890 - val_loss: 0.4649 - val_categorical_accuracy: 0.8917\n",
      "Epoch 14/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.0242 - categorical_accuracy: 0.9903 - val_loss: 0.4368 - val_categorical_accuracy: 0.9033\n",
      "Epoch 15/60\n",
      "3000/3000 [==============================] - 1s 331us/step - loss: 0.0262 - categorical_accuracy: 0.9917 - val_loss: 0.4050 - val_categorical_accuracy: 0.9050\n",
      "Epoch 16/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0417 - categorical_accuracy: 0.9867 - val_loss: 0.4899 - val_categorical_accuracy: 0.8875\n",
      "Epoch 17/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.0331 - categorical_accuracy: 0.9873 - val_loss: 0.4121 - val_categorical_accuracy: 0.9067\n",
      "Epoch 18/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0083 - categorical_accuracy: 0.9977 - val_loss: 0.5028 - val_categorical_accuracy: 0.8933\n",
      "Epoch 19/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.0124 - categorical_accuracy: 0.9957 - val_loss: 0.4730 - val_categorical_accuracy: 0.9067\n",
      "Epoch 20/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.0263 - categorical_accuracy: 0.9883 - val_loss: 0.4287 - val_categorical_accuracy: 0.8842\n",
      "Epoch 21/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0246 - categorical_accuracy: 0.9923 - val_loss: 0.4919 - val_categorical_accuracy: 0.8975\n",
      "Epoch 22/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0100 - categorical_accuracy: 0.9967 - val_loss: 0.5181 - val_categorical_accuracy: 0.9083\n",
      "Epoch 23/60\n",
      "3000/3000 [==============================] - 1s 337us/step - loss: 0.0148 - categorical_accuracy: 0.9937 - val_loss: 0.5657 - val_categorical_accuracy: 0.8950\n",
      "Epoch 24/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0107 - categorical_accuracy: 0.9967 - val_loss: 0.6228 - val_categorical_accuracy: 0.8900\n",
      "Epoch 25/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.0085 - categorical_accuracy: 0.9967 - val_loss: 0.5803 - val_categorical_accuracy: 0.9108\n",
      "Epoch 26/60\n",
      "3000/3000 [==============================] - 1s 353us/step - loss: 0.0303 - categorical_accuracy: 0.9910 - val_loss: 0.4745 - val_categorical_accuracy: 0.9117\n",
      "Epoch 27/60\n",
      "3000/3000 [==============================] - 1s 345us/step - loss: 0.0143 - categorical_accuracy: 0.9937 - val_loss: 0.5417 - val_categorical_accuracy: 0.9017\n",
      "Epoch 28/60\n",
      "3000/3000 [==============================] - 1s 335us/step - loss: 0.0236 - categorical_accuracy: 0.9923 - val_loss: 0.5681 - val_categorical_accuracy: 0.9050\n",
      "Epoch 29/60\n",
      "3000/3000 [==============================] - 1s 331us/step - loss: 0.0255 - categorical_accuracy: 0.9897 - val_loss: 0.4789 - val_categorical_accuracy: 0.9117\n",
      "Epoch 30/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0075 - categorical_accuracy: 0.9977 - val_loss: 0.4756 - val_categorical_accuracy: 0.9133\n",
      "Epoch 31/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0103 - categorical_accuracy: 0.9973 - val_loss: 0.5945 - val_categorical_accuracy: 0.9042\n",
      "Epoch 32/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.0277 - categorical_accuracy: 0.9920 - val_loss: 0.5015 - val_categorical_accuracy: 0.9125\n",
      "Epoch 33/60\n",
      "3000/3000 [==============================] - 1s 331us/step - loss: 0.0245 - categorical_accuracy: 0.9917 - val_loss: 0.6835 - val_categorical_accuracy: 0.8858\n",
      "Epoch 34/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0052 - categorical_accuracy: 0.9987 - val_loss: 0.5320 - val_categorical_accuracy: 0.9108\n",
      "Epoch 35/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0081 - categorical_accuracy: 0.9980 - val_loss: 0.4756 - val_categorical_accuracy: 0.9083\n",
      "Epoch 36/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.0063 - categorical_accuracy: 0.9983 - val_loss: 0.5734 - val_categorical_accuracy: 0.9000\n",
      "Epoch 37/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0036 - categorical_accuracy: 0.9993 - val_loss: 0.5948 - val_categorical_accuracy: 0.9067\n",
      "Epoch 38/60\n",
      "3000/3000 [==============================] - 1s 337us/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 0.6135 - val_categorical_accuracy: 0.9075\n",
      "Epoch 39/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 0.7165 - val_categorical_accuracy: 0.9033\n",
      "Epoch 40/60\n",
      "3000/3000 [==============================] - 1s 331us/step - loss: 0.0102 - categorical_accuracy: 0.9967 - val_loss: 0.7092 - val_categorical_accuracy: 0.8950\n",
      "Epoch 41/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0223 - categorical_accuracy: 0.9923 - val_loss: 0.8443 - val_categorical_accuracy: 0.8983\n",
      "Epoch 42/60\n",
      "3000/3000 [==============================] - 1s 340us/step - loss: 0.0456 - categorical_accuracy: 0.9823 - val_loss: 0.5919 - val_categorical_accuracy: 0.8900\n",
      "Epoch 43/60\n",
      "3000/3000 [==============================] - 1s 337us/step - loss: 0.0189 - categorical_accuracy: 0.9943 - val_loss: 0.6194 - val_categorical_accuracy: 0.9050\n",
      "Epoch 44/60\n",
      "3000/3000 [==============================] - 1s 331us/step - loss: 0.0040 - categorical_accuracy: 0.9987 - val_loss: 0.7252 - val_categorical_accuracy: 0.8950\n",
      "Epoch 45/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.0266 - categorical_accuracy: 0.9913 - val_loss: 0.4692 - val_categorical_accuracy: 0.9117\n",
      "Epoch 46/60\n",
      "3000/3000 [==============================] - 1s 335us/step - loss: 0.0045 - categorical_accuracy: 0.9983 - val_loss: 0.5764 - val_categorical_accuracy: 0.9083\n",
      "Epoch 47/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0049 - categorical_accuracy: 0.9990 - val_loss: 0.7385 - val_categorical_accuracy: 0.8983\n",
      "Epoch 48/60\n",
      "3000/3000 [==============================] - 1s 334us/step - loss: 0.0093 - categorical_accuracy: 0.9967 - val_loss: 0.6354 - val_categorical_accuracy: 0.9058\n",
      "Epoch 49/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.0031 - categorical_accuracy: 0.9993 - val_loss: 0.7220 - val_categorical_accuracy: 0.8975\n",
      "Epoch 50/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 0.7126 - val_categorical_accuracy: 0.9117\n",
      "Epoch 51/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.0013 - categorical_accuracy: 0.9997 - val_loss: 0.7091 - val_categorical_accuracy: 0.9050\n",
      "Epoch 52/60\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.0333 - categorical_accuracy: 0.9913 - val_loss: 0.7443 - val_categorical_accuracy: 0.8842\n",
      "Epoch 53/60\n",
      "3000/3000 [==============================] - 1s 334us/step - loss: 0.0242 - categorical_accuracy: 0.9920 - val_loss: 0.7005 - val_categorical_accuracy: 0.9008\n",
      "Epoch 54/60\n",
      "3000/3000 [==============================] - 1s 335us/step - loss: 0.0172 - categorical_accuracy: 0.9937 - val_loss: 0.8118 - val_categorical_accuracy: 0.8858\n",
      "Epoch 55/60\n",
      "3000/3000 [==============================] - 1s 331us/step - loss: 0.0150 - categorical_accuracy: 0.9943 - val_loss: 0.5181 - val_categorical_accuracy: 0.9075\n",
      "Epoch 56/60\n",
      "3000/3000 [==============================] - 1s 331us/step - loss: 0.0089 - categorical_accuracy: 0.9967 - val_loss: 0.8589 - val_categorical_accuracy: 0.8800\n",
      "Epoch 57/60\n",
      "3000/3000 [==============================] - 1s 335us/step - loss: 0.0209 - categorical_accuracy: 0.9923 - val_loss: 0.6925 - val_categorical_accuracy: 0.9033\n",
      "Epoch 58/60\n",
      "3000/3000 [==============================] - 1s 347us/step - loss: 0.0031 - categorical_accuracy: 0.9987 - val_loss: 0.6841 - val_categorical_accuracy: 0.9050\n",
      "Epoch 59/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 2.8890e-04 - categorical_accuracy: 1.0000 - val_loss: 0.7342 - val_categorical_accuracy: 0.9033\n",
      "Epoch 60/60\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.0045 - categorical_accuracy: 0.9987 - val_loss: 0.6702 - val_categorical_accuracy: 0.9050\n",
      "Top model weights trained\n",
      "Top model built\n",
      "Final model built\n",
      "Found 3000 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saniea/miniconda2/envs/thesis/lib/python3.6/site-packages/ipykernel_launcher.py:90: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 3 classes.\n",
      "Beginning model fitting\n",
      "Epoch 1/60\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.4010 - categorical_accuracy: 0.9230 - val_loss: 0.5885 - val_categorical_accuracy: 0.9075\n",
      "Epoch 2/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.3589 - categorical_accuracy: 0.9297 - val_loss: 0.5716 - val_categorical_accuracy: 0.9058\n",
      "Epoch 3/60\n",
      "200/200 [==============================] - 35s 175ms/step - loss: 0.2964 - categorical_accuracy: 0.9347 - val_loss: 0.5496 - val_categorical_accuracy: 0.9058\n",
      "Epoch 4/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.3355 - categorical_accuracy: 0.9233 - val_loss: 0.5357 - val_categorical_accuracy: 0.9058\n",
      "Epoch 5/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2724 - categorical_accuracy: 0.9357 - val_loss: 0.5227 - val_categorical_accuracy: 0.9067\n",
      "Epoch 6/60\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.3007 - categorical_accuracy: 0.9343 - val_loss: 0.5102 - val_categorical_accuracy: 0.9058\n",
      "Epoch 7/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2849 - categorical_accuracy: 0.9363 - val_loss: 0.5016 - val_categorical_accuracy: 0.9075\n",
      "Epoch 8/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2416 - categorical_accuracy: 0.9423 - val_loss: 0.4920 - val_categorical_accuracy: 0.9092\n",
      "Epoch 9/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2385 - categorical_accuracy: 0.9337 - val_loss: 0.4843 - val_categorical_accuracy: 0.9108\n",
      "Epoch 10/60\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.2754 - categorical_accuracy: 0.9330 - val_loss: 0.4746 - val_categorical_accuracy: 0.9117\n",
      "Epoch 11/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2699 - categorical_accuracy: 0.9303 - val_loss: 0.4657 - val_categorical_accuracy: 0.9117\n",
      "Epoch 12/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2427 - categorical_accuracy: 0.9310 - val_loss: 0.4612 - val_categorical_accuracy: 0.9117\n",
      "Epoch 13/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2292 - categorical_accuracy: 0.9380 - val_loss: 0.4600 - val_categorical_accuracy: 0.9108\n",
      "Epoch 14/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2268 - categorical_accuracy: 0.9360 - val_loss: 0.4553 - val_categorical_accuracy: 0.9117\n",
      "Epoch 15/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2401 - categorical_accuracy: 0.9343 - val_loss: 0.4505 - val_categorical_accuracy: 0.9117\n",
      "Epoch 16/60\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.2152 - categorical_accuracy: 0.9387 - val_loss: 0.4475 - val_categorical_accuracy: 0.9125\n",
      "Epoch 17/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2388 - categorical_accuracy: 0.9423 - val_loss: 0.4403 - val_categorical_accuracy: 0.9142\n",
      "Epoch 18/60\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.2312 - categorical_accuracy: 0.9360 - val_loss: 0.4366 - val_categorical_accuracy: 0.9142\n",
      "Epoch 19/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1925 - categorical_accuracy: 0.9453 - val_loss: 0.4337 - val_categorical_accuracy: 0.9125\n",
      "Epoch 20/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1821 - categorical_accuracy: 0.9473 - val_loss: 0.4307 - val_categorical_accuracy: 0.9108\n",
      "Epoch 21/60\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.1981 - categorical_accuracy: 0.9377 - val_loss: 0.4275 - val_categorical_accuracy: 0.9125\n",
      "Epoch 22/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1970 - categorical_accuracy: 0.9407 - val_loss: 0.4253 - val_categorical_accuracy: 0.9133\n",
      "Epoch 23/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2137 - categorical_accuracy: 0.9393 - val_loss: 0.4228 - val_categorical_accuracy: 0.9108\n",
      "Epoch 24/60\n",
      "200/200 [==============================] - 35s 175ms/step - loss: 0.2021 - categorical_accuracy: 0.9390 - val_loss: 0.4181 - val_categorical_accuracy: 0.9125\n",
      "Epoch 25/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.2081 - categorical_accuracy: 0.9387 - val_loss: 0.4184 - val_categorical_accuracy: 0.9125\n",
      "Epoch 26/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1840 - categorical_accuracy: 0.9403 - val_loss: 0.4175 - val_categorical_accuracy: 0.9117\n",
      "Epoch 27/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1831 - categorical_accuracy: 0.9377 - val_loss: 0.4143 - val_categorical_accuracy: 0.9108\n",
      "Epoch 28/60\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.2004 - categorical_accuracy: 0.9403 - val_loss: 0.4126 - val_categorical_accuracy: 0.9100\n",
      "Epoch 29/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1975 - categorical_accuracy: 0.9407 - val_loss: 0.4104 - val_categorical_accuracy: 0.9117\n",
      "Epoch 30/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1812 - categorical_accuracy: 0.9470 - val_loss: 0.4065 - val_categorical_accuracy: 0.9142\n",
      "Epoch 31/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1777 - categorical_accuracy: 0.9503 - val_loss: 0.4048 - val_categorical_accuracy: 0.9133\n",
      "Epoch 32/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1759 - categorical_accuracy: 0.9440 - val_loss: 0.4011 - val_categorical_accuracy: 0.9133\n",
      "Epoch 33/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1802 - categorical_accuracy: 0.9470 - val_loss: 0.3994 - val_categorical_accuracy: 0.9133\n",
      "Epoch 34/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1655 - categorical_accuracy: 0.9477 - val_loss: 0.3980 - val_categorical_accuracy: 0.9125\n",
      "Epoch 35/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1849 - categorical_accuracy: 0.9450 - val_loss: 0.3963 - val_categorical_accuracy: 0.9125\n",
      "Epoch 36/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1766 - categorical_accuracy: 0.9437 - val_loss: 0.3936 - val_categorical_accuracy: 0.9108\n",
      "Epoch 37/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1892 - categorical_accuracy: 0.9440 - val_loss: 0.3927 - val_categorical_accuracy: 0.9142\n",
      "Epoch 38/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1567 - categorical_accuracy: 0.9497 - val_loss: 0.3906 - val_categorical_accuracy: 0.9133\n",
      "Epoch 39/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1534 - categorical_accuracy: 0.9477 - val_loss: 0.3885 - val_categorical_accuracy: 0.9133\n",
      "Epoch 40/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1557 - categorical_accuracy: 0.9493 - val_loss: 0.3879 - val_categorical_accuracy: 0.9125\n",
      "Epoch 41/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1696 - categorical_accuracy: 0.9473 - val_loss: 0.3875 - val_categorical_accuracy: 0.9125\n",
      "Epoch 42/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1763 - categorical_accuracy: 0.9420 - val_loss: 0.3860 - val_categorical_accuracy: 0.9117\n",
      "Epoch 43/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1715 - categorical_accuracy: 0.9473 - val_loss: 0.3855 - val_categorical_accuracy: 0.9125\n",
      "Epoch 44/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1585 - categorical_accuracy: 0.9493 - val_loss: 0.3843 - val_categorical_accuracy: 0.9125\n",
      "Epoch 45/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1667 - categorical_accuracy: 0.9490 - val_loss: 0.3792 - val_categorical_accuracy: 0.9125\n",
      "Epoch 46/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1718 - categorical_accuracy: 0.9457 - val_loss: 0.3804 - val_categorical_accuracy: 0.9117\n",
      "Epoch 47/60\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.1474 - categorical_accuracy: 0.9527 - val_loss: 0.3782 - val_categorical_accuracy: 0.9108\n",
      "Epoch 48/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1718 - categorical_accuracy: 0.9490 - val_loss: 0.3757 - val_categorical_accuracy: 0.9117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1414 - categorical_accuracy: 0.9520 - val_loss: 0.3760 - val_categorical_accuracy: 0.9108\n",
      "Epoch 50/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1474 - categorical_accuracy: 0.9580 - val_loss: 0.3741 - val_categorical_accuracy: 0.9100\n",
      "Epoch 51/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1330 - categorical_accuracy: 0.9547 - val_loss: 0.3725 - val_categorical_accuracy: 0.9100\n",
      "Epoch 52/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1358 - categorical_accuracy: 0.9540 - val_loss: 0.3751 - val_categorical_accuracy: 0.9158\n",
      "Epoch 53/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1666 - categorical_accuracy: 0.9457 - val_loss: 0.3721 - val_categorical_accuracy: 0.9125\n",
      "Epoch 54/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1642 - categorical_accuracy: 0.9507 - val_loss: 0.3716 - val_categorical_accuracy: 0.9125\n",
      "Epoch 55/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1472 - categorical_accuracy: 0.9527 - val_loss: 0.3718 - val_categorical_accuracy: 0.9117\n",
      "Epoch 56/60\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.1544 - categorical_accuracy: 0.9457 - val_loss: 0.3726 - val_categorical_accuracy: 0.9125\n",
      "Epoch 57/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1635 - categorical_accuracy: 0.9533 - val_loss: 0.3703 - val_categorical_accuracy: 0.9117\n",
      "Epoch 58/60\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.1445 - categorical_accuracy: 0.9527 - val_loss: 0.3678 - val_categorical_accuracy: 0.9125\n",
      "Epoch 59/60\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.1336 - categorical_accuracy: 0.9533 - val_loss: 0.3670 - val_categorical_accuracy: 0.9142\n",
      "Epoch 60/60\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.1442 - categorical_accuracy: 0.9490 - val_loss: 0.3681 - val_categorical_accuracy: 0.9133\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 3)                 2098179   \n",
      "=================================================================\n",
      "Total params: 16,812,867\n",
      "Trainable params: 9,177,603\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 15\n",
    "top_model_weights_path = 'fc_model.h5'\n",
    "vgg_model = save_bottleneck_features()\n",
    "train_weights()\n",
    "top_model = build_top_model(vgg_model)\n",
    "composite_model(vgg_model, top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_categorical_accuracy', 'loss', 'categorical_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "with open('model_history', 'rb') as f:\n",
    "    hist = pickle.load(f)\n",
    "\n",
    "print (hist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saniea/miniconda2/envs/thesis/lib/python3.6/site-packages/matplotlib/figure.py:2299: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  warnings.warn(\"This figure includes Axes that are not compatible \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('model_history', 'rb') as f:\n",
    "    hist = pickle.load(f)\n",
    "\n",
    "key2name = {'categorical_accuracy':'Accuracy', 'loss':'Loss', \n",
    "    'val_categorical_accuracy':'Validation Accuracy', 'val_loss':'Validation Loss'}\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# things = ['categorical_accuracy','loss','val_categorical_accuracy','val_loss']\n",
    "things = ['loss','val_loss']\n",
    "for i,thing in enumerate(things):\n",
    "    trace = hist[thing]\n",
    "#     plt.subplot(2,2,i+1)\n",
    "    plt.plot(range(len(trace)),trace)\n",
    "    plt.title(key2name[thing])\n",
    "\n",
    "# fig.set_tight_layout(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
