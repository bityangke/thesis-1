{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "# import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "traindir = '/home/saniea/imagedata/cifar10/'\n",
    "valdir = '/home/saniea/imagedata/cifar10/test_batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def convert(data):\n",
    "    # magic recursive function\n",
    "    if isinstance(data, bytes):      return data.decode('ascii')\n",
    "    if isinstance(data, dict):       return dict(map(convert, data.items()))\n",
    "    if isinstance(data, tuple):      return map(convert, data)\n",
    "    if isinstance(data, list):       return list(map(convert, data))\n",
    "    return data\n",
    "\n",
    "# TODO: function to print image if needed\n",
    "# X = X.reshape(50000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")\n",
    "# # X = X.reshape(50000, 3, 32, 32)\n",
    "# # X = X.transpose(0, 2, 3, 1)\n",
    "# plt.imshow(X[5], interpolation='none')\n",
    "# print(label_names[int(Y[5])])\n",
    "# plt.figure()\n",
    "# plt.imshow(X[7])\n",
    "# print(label_names[int(Y[7])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (50000, 3072)\n",
      "Training labels size: 50000\n",
      "Validation data size: (10000, 3072)\n",
      "Validation labels size: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, filenames in os.walk(traindir):\n",
    "    files = filenames\n",
    "# print(files)\n",
    "\n",
    "data = [] # list of all training data files to be unpacked, total 50k images\n",
    "[data.append(file) for file in files if (file.startswith('data') == True)]\n",
    "data.sort()\n",
    "# print(data)\n",
    "\n",
    "test = unpickle(str(traindir+'data_batch_1'))\n",
    "# print(test.keys())\n",
    "\n",
    "X = np.empty((50000,3072))\n",
    "Y = np.empty((50000))\n",
    "batch_len = 10000\n",
    "\n",
    "for i, batch in enumerate (data):\n",
    "    batch = unpickle(str(traindir+batch))\n",
    "    current_batch = {k.decode() : v for k, v in batch.items()}\n",
    "#     batch_data = current_batch[b'data']\n",
    "#     batch_labels = current_batch[b'labels']\n",
    "#     j = i + 1\n",
    "    X[batch_len*i:batch_len*(i+1)] = current_batch['data']\n",
    "    Y[batch_len*i:batch_len*(i+1)] = current_batch['labels']\n",
    "    \n",
    "    # i = 0, index(j) = i + 1 = 1 --> 00000 - 10000\n",
    "    # i = 1, index(j) = i + 1 = 2 --> 10000 - 20000\n",
    "    # i = 2, index(j) = i + 1 = 3 --> 20000 - 30000\n",
    "\n",
    "meta_b = unpickle(str(traindir+'batches.meta'))\n",
    "meta = {k.decode() : v for k, v in meta_b.items()}\n",
    "label_names = meta['label_names']\n",
    "\n",
    "# test_batch = unpickle(valdir)\n",
    "val_batch = {k.decode():v for k, v in unpickle(valdir).items()}\n",
    "X_val = val_batch['data']\n",
    "Y_val = val_batch['labels']\n",
    "    \n",
    "print('Training data size: ' + str(X.shape) + '\\n'\n",
    "     +'Training labels size: ' + str(len(Y)) + '\\n'\n",
    "     +'Validation data size: ' + str(X_val.shape) + '\\n'\n",
    "     +'Validation labels size: ' + str(len(Y_val)) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
